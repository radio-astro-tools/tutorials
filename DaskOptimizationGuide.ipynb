{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "599a6685-687c-4984-8fa6-acc363d80e7c",
   "metadata": {},
   "source": [
    "# Guide to dask chunking in Spectral-Cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577b4484-90b7-4882-9108-18c9673f3aa6",
   "metadata": {},
   "source": [
    "Dask is a data processing framework integrated into spectral-cube that enables parallel processing of larger-than-memory cubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015d2f43-99aa-4d3b-abb3-ff713be64fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.utils.data import download_file\n",
    "from spectral_cube import SpectralCube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4020b148-23d6-4028-8e1e-91ef6854b3c5",
   "metadata": {},
   "source": [
    "We download a cube from the MAPS survey:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f971b2-588a-4025-bdf6-5dd001d34742",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = download_file('ftp://ftp.cv.nrao.edu/NRAO-staff/rloomis/MAPS/HD_163296/images/CO/0.15arcsec/HD_163296_CO_220GHz.0.15arcsec.JvMcorr.image.pbcor.fits', cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06788b3c-2af5-4859-ac0b-11320b340d4a",
   "metadata": {},
   "source": [
    "We load the cube using the `dask` backend, which allows for some parallelization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82107ea-c0f2-4b23-a8ae-d8b140f11205",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = SpectralCube.read(filename, use_dask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d692a951-f3a5-41cf-8484-638003ededac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a34ee0f-b007-41ab-b6cb-a9b839e6ec85",
   "metadata": {},
   "source": [
    "# Dask Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c7cb0-cf99-4e1c-beb9-6804eb39e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use _data, it will cause problems!  This is _purely_ for visualization purposes\n",
    "cube._data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708cc844-79de-43c6-afed-e5aca6e42a5a",
   "metadata": {},
   "source": [
    "Dask data can be 'chunked' to optimize oeprations along different directions.  \n",
    "\n",
    "For example, this first 'rechunk' will load full spectra into memory, but will break the cube into sub-cubes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f796472-9991-46ac-be00-bbc05e1eb37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rechunked_cube_spectrally = cube.rechunk((-1,'auto','auto'))\n",
    "rechunked_cube_spectrally._data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42700676-11a7-41a7-9e89-ed439bf468f0",
   "metadata": {},
   "source": [
    "While this chunking will grab sub-cubes of size 8 in the spectral direction, but the full image in the spatial directions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9892d3-2ffb-43aa-ad20-1e3718d51b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rechunked_cube_spatially = cube.rechunk(('auto',-1,-1))\n",
    "rechunked_cube_spatially._data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc529fa-77b9-4e26-8a21-fafd3fcf78e6",
   "metadata": {},
   "source": [
    "You can also enforce individual small chunks if you want to ensure every operation fits in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a513e1e-3bfe-4e4c-9da1-fbc649c66024",
   "metadata": {},
   "outputs": [],
   "source": [
    "rechunked_cube = cube.rechunk((25, 512, 512))\n",
    "rechunked_cube._data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd61f9-618e-440a-a819-0b81475657a6",
   "metadata": {},
   "source": [
    "You can control dask's functionality using dask directly.  The preferred approach is to use context managers, e.g., for progressbars and schedulers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca00c80-c092-4e7a-848e-c1951362040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "pbar = ProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79de51-1bb6-400a-8d2b-faf975786696",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pbar:\n",
    "    cube.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630307f-c806-4c1c-b5f3-96119cd1210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the synchronous scheduler is the default\n",
    "# our default chunking uses 16 chunks\n",
    "with cube.use_dask_scheduler('synchronous'):\n",
    "    with pbar:\n",
    "        cube.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf0c40b-fd67-4bc0-84ae-ad0eb9a78703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this version is slower if we load the whole cube into memory but potentially uses less memory & is parallel\n",
    "with cube.use_dask_scheduler('threads', num_workers=8):\n",
    "    with pbar:\n",
    "        cube.rechunk((25,512,512)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb12da5-2107-4eaa-9201-e4651425ebe5",
   "metadata": {},
   "source": [
    "For some huge cubes & operations, it is necessary to save intermediate steps to disk.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f9331-9ee1-4e3b-be17-c9028734ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pbar:\n",
    "    spectrally_chunked = cube.rechunk((-1,'auto','auto'), save_to_tmp_dir=True)\n",
    "spectrally_chunked._data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae7b44b-369e-4ea0-80ad-8e67e6240ecf",
   "metadata": {},
   "source": [
    "That `_data` is now saved on disk, which can be necessary for huge operations along dimensions that are hard to store in memory.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
